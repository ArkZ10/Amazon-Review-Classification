{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"id\":\"paGt1cLCZo9-\",\"outputId\":\"8fedc909-196c-48a9-bb8a-6edbc9237e7d\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T12:01:39.707893Z\",\"iopub.execute_input\":\"2024-05-25T12:01:39.708889Z\",\"iopub.status.idle\":\"2024-05-25T12:01:56.969531Z\",\"shell.execute_reply.started\":\"2024-05-25T12:01:39.708844Z\",\"shell.execute_reply\":\"2024-05-25T12:01:56.968434Z\"}}\n!pip install virtualenv\n!virtualenv mlops-tfx\n\n# %% [code] {\"id\":\"Pw8VE4zhO8fd\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T12:01:56.971605Z\",\"iopub.execute_input\":\"2024-05-25T12:01:56.971899Z\",\"iopub.status.idle\":\"2024-05-25T12:01:57.987885Z\",\"shell.execute_reply.started\":\"2024-05-25T12:01:56.971873Z\",\"shell.execute_reply\":\"2024-05-25T12:01:57.986562Z\"}}\n!source mlops-tfx/bin/activate\n\n# %% [code] {\"id\":\"T8FpFCx8PB_k\",\"outputId\":\"8a15b729-ca8f-45aa-bf4e-f45a82ce1c32\",\"scrolled\":true,\"execution\":{\"iopub.status.busy\":\"2024-05-25T12:01:57.989634Z\",\"iopub.execute_input\":\"2024-05-25T12:01:57.989975Z\",\"iopub.status.idle\":\"2024-05-25T13:36:17.652035Z\",\"shell.execute_reply.started\":\"2024-05-25T12:01:57.989935Z\",\"shell.execute_reply\":\"2024-05-25T13:36:17.650935Z\"}}\n!pip install jupyter scikit-learn tensorflow tfx==1.14.0 flask joblib\n\n# %% [code] {\"id\":\"0raP6ANZP8H5\",\"outputId\":\"ffc16ffa-e038-4342-e391-c4e9d3a01171\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T13:36:17.655113Z\",\"iopub.execute_input\":\"2024-05-25T13:36:17.655455Z\",\"iopub.status.idle\":\"2024-05-25T13:36:21.825605Z\",\"shell.execute_reply.started\":\"2024-05-25T13:36:17.655424Z\",\"shell.execute_reply\":\"2024-05-25T13:36:21.824010Z\"}}\n!git clone https://github.com/ArkZ10/Amazon-Review-Classification.git\n\n# %% [markdown] {\"id\":\"56BUZSgdUqxj\"}\n# # DATA PREPARATION\n\n# %% [code] {\"id\":\"rGecabKYQ-Ni\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T13:36:21.827109Z\",\"iopub.execute_input\":\"2024-05-25T13:36:21.827824Z\",\"iopub.status.idle\":\"2024-05-25T13:36:22.529265Z\",\"shell.execute_reply.started\":\"2024-05-25T13:36:21.827788Z\",\"shell.execute_reply\":\"2024-05-25T13:36:22.528467Z\"}}\nimport pandas as pd\nimport os\ndf = pd.read_csv(\"/kaggle/working/Amazon-Review-Classification/Dataset/Dataset.csv\")\n\n# %% [code] {\"id\":\"zHA41PejTbNj\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T13:36:22.530420Z\",\"iopub.execute_input\":\"2024-05-25T13:36:22.530710Z\",\"iopub.status.idle\":\"2024-05-25T13:36:22.653473Z\",\"shell.execute_reply.started\":\"2024-05-25T13:36:22.530685Z\",\"shell.execute_reply\":\"2024-05-25T13:36:22.652017Z\"}}\ndf = df[[\"Text\", \"Cat1\"]]\n\n# %% [code] {\"id\":\"lBaxT4K7TqFD\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T13:36:22.655812Z\",\"iopub.execute_input\":\"2024-05-25T13:36:22.656587Z\",\"iopub.status.idle\":\"2024-05-25T13:36:22.763666Z\",\"shell.execute_reply.started\":\"2024-05-25T13:36:22.656557Z\",\"shell.execute_reply\":\"2024-05-25T13:36:22.762558Z\"}}\ndf = df.rename(columns={\"Cat1\" : \"Label\"})\n\n# %% [code] {\"id\":\"ewS3R2onwZZH\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T13:36:22.765183Z\",\"iopub.execute_input\":\"2024-05-25T13:36:22.766191Z\",\"iopub.status.idle\":\"2024-05-25T13:36:22.849242Z\",\"shell.execute_reply.started\":\"2024-05-25T13:36:22.766151Z\",\"shell.execute_reply\":\"2024-05-25T13:36:22.847190Z\"}}\ndf = df.dropna()\n\n# %% [code] {\"id\":\"X6us7yCGuxcp\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T13:36:22.851011Z\",\"iopub.execute_input\":\"2024-05-25T13:36:22.851366Z\",\"iopub.status.idle\":\"2024-05-25T13:36:22.939588Z\",\"shell.execute_reply.started\":\"2024-05-25T13:36:22.851328Z\",\"shell.execute_reply\":\"2024-05-25T13:36:22.938485Z\"}}\ndf = df.groupby('Label', group_keys=False).apply(lambda x: x.sample(min(len(x), 500)))\n\n# %% [code] {\"id\":\"XV4FuH5b4-tK\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T13:36:22.944739Z\",\"iopub.execute_input\":\"2024-05-25T13:36:22.945070Z\",\"iopub.status.idle\":\"2024-05-25T13:36:22.967717Z\",\"shell.execute_reply.started\":\"2024-05-25T13:36:22.945042Z\",\"shell.execute_reply\":\"2024-05-25T13:36:22.966512Z\"}}\ndf['Label'] = df['Label'].str.replace(' ', '_')\nclasses_to_keep = ['toys_games', 'beauty', 'grocery_gourmet_food', 'health_personal_care']\ndf = df[df['Label'].isin(classes_to_keep)]\ndf['Label'] = df['Label'].astype('category').cat.codes\n\n# %% [code] {\"id\":\"FbqBvNWgUzXG\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T13:36:22.969242Z\",\"iopub.execute_input\":\"2024-05-25T13:36:22.970240Z\",\"iopub.status.idle\":\"2024-05-25T13:36:23.067192Z\",\"shell.execute_reply.started\":\"2024-05-25T13:36:22.970173Z\",\"shell.execute_reply\":\"2024-05-25T13:36:23.065863Z\"}}\nDATA_PATH = 'Data'\n\nif not os.path.exists(DATA_PATH):\n    os.makedirs(DATA_PATH)\n\ndf.to_csv(os.path.join(DATA_PATH, 'review.csv'), index=False)\n\n# %% [code] {\"id\":\"9Mm3_L28VJtB\",\"outputId\":\"06ffcd97-8cfe-4311-bbee-22faace9ab47\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T13:36:23.068743Z\",\"iopub.execute_input\":\"2024-05-25T13:36:23.069166Z\",\"iopub.status.idle\":\"2024-05-25T13:36:23.086445Z\",\"shell.execute_reply.started\":\"2024-05-25T13:36:23.069129Z\",\"shell.execute_reply\":\"2024-05-25T13:36:23.085434Z\"}}\ndf.head(2)\n\n# %% [code] {\"id\":\"idFEEnaD4xjy\",\"outputId\":\"c3f751ac-dd76-4558-c94e-c27596d0765c\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T13:36:23.087996Z\",\"iopub.execute_input\":\"2024-05-25T13:36:23.088600Z\",\"iopub.status.idle\":\"2024-05-25T13:36:23.126585Z\",\"shell.execute_reply.started\":\"2024-05-25T13:36:23.088566Z\",\"shell.execute_reply\":\"2024-05-25T13:36:23.125439Z\"}}\ndf['Label'].value_counts()\n\n# %% [markdown] {\"id\":\"8rzw_lxoUuCa\"}\n# # Import Library\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-05-25T13:36:23.128070Z\",\"iopub.execute_input\":\"2024-05-25T13:36:23.128698Z\",\"iopub.status.idle\":\"2024-05-25T13:36:36.506437Z\",\"shell.execute_reply.started\":\"2024-05-25T13:36:23.128658Z\",\"shell.execute_reply\":\"2024-05-25T13:36:36.505307Z\"}}\n!pip install tensorflow\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-05-25T13:36:36.508096Z\",\"iopub.execute_input\":\"2024-05-25T13:36:36.508452Z\",\"iopub.status.idle\":\"2024-05-25T13:37:03.050402Z\",\"shell.execute_reply.started\":\"2024-05-25T13:36:36.508421Z\",\"shell.execute_reply\":\"2024-05-25T13:37:03.049524Z\"}}\nimport os\nimport pandas as pd\n\nimport tensorflow as tf\nimport tensorflow_model_analysis as tfma\n\nfrom tfx.components import CsvExampleGen, StatisticsGen, SchemaGen, ExampleValidator\nfrom tfx.components import Transform, Trainer, Tuner, Evaluator, Pusher\nfrom tfx.proto import example_gen_pb2, trainer_pb2, pusher_pb2\nfrom tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\nfrom tfx.dsl.components.common.resolver import Resolver\nfrom tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import LatestBlessedModelStrategy\nfrom tfx.types import Channel\nfrom tfx.types.standard_artifacts import Model, ModelBlessing\nimport pprint\n\npp = pprint.PrettyPrinter()\n\n# %% [markdown] {\"id\":\"x74YBEe7Pzvf\"}\n# # PIPELINE\n\n# %% [markdown] {\"id\":\"uH5Nb3FdPy0M\"}\n# ## 1.1 Data Ingestion (ExampleGen)\n\n# %% [markdown]\n# \n\n# %% [code] {\"id\":\"n8ofl7F2PF6J\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T13:37:03.051532Z\",\"iopub.execute_input\":\"2024-05-25T13:37:03.052108Z\",\"iopub.status.idle\":\"2024-05-25T13:37:03.057556Z\",\"shell.execute_reply.started\":\"2024-05-25T13:37:03.052078Z\",\"shell.execute_reply\":\"2024-05-25T13:37:03.056441Z\"}}\nPIPELINE_NAME = \"review-classification\"\nSCHEMA_PIPELINE_NAME = \"review-classification-schema\"\n\nPIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\nMETADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n\nSERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n\n# %% [code] {\"id\":\"z0AVpOpTWPTo\",\"outputId\":\"4f5fe8cc-f009-4dc9-f71d-c51a79c61500\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T13:37:03.058980Z\",\"iopub.execute_input\":\"2024-05-25T13:37:03.059682Z\",\"iopub.status.idle\":\"2024-05-25T13:37:03.077547Z\",\"shell.execute_reply.started\":\"2024-05-25T13:37:03.059628Z\",\"shell.execute_reply\":\"2024-05-25T13:37:03.076656Z\"}}\ninteractive_context = InteractiveContext(pipeline_root=PIPELINE_ROOT)\n\n# %% [code] {\"id\":\"lySK4L5ATSng\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T13:37:03.078741Z\",\"iopub.execute_input\":\"2024-05-25T13:37:03.079468Z\",\"iopub.status.idle\":\"2024-05-25T13:37:03.087098Z\",\"shell.execute_reply.started\":\"2024-05-25T13:37:03.079440Z\",\"shell.execute_reply\":\"2024-05-25T13:37:03.086147Z\"}}\nDATA_ROOT = 'Data'\n\n# %% [code] {\"id\":\"O47X8urcVec3\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T13:37:03.088260Z\",\"iopub.execute_input\":\"2024-05-25T13:37:03.088583Z\",\"iopub.status.idle\":\"2024-05-25T13:37:03.097192Z\",\"shell.execute_reply.started\":\"2024-05-25T13:37:03.088559Z\",\"shell.execute_reply\":\"2024-05-25T13:37:03.096345Z\"}}\noutput = example_gen_pb2.Output(\n    split_config = example_gen_pb2.SplitConfig(splits=[\n        example_gen_pb2.SplitConfig.Split(name=\"train\", hash_buckets=8),\n        example_gen_pb2.SplitConfig.Split(name=\"eval\", hash_buckets=2)\n    ])\n)\nexample_gen = CsvExampleGen(input_base=DATA_ROOT, output_config=output)\n\n# %% [code] {\"id\":\"lC1vIm6IVwM4\",\"outputId\":\"a266ccba-c27c-400a-a67c-c95435606965\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T13:37:03.098397Z\",\"iopub.execute_input\":\"2024-05-25T13:37:03.098738Z\",\"iopub.status.idle\":\"2024-05-25T13:37:04.991072Z\",\"shell.execute_reply.started\":\"2024-05-25T13:37:03.098703Z\",\"shell.execute_reply\":\"2024-05-25T13:37:04.990136Z\"}}\ninteractive_context.run(example_gen)\n\n# %% [code] {\"id\":\"WBuO28eYWVIm\",\"outputId\":\"acffad7a-03e2-49b8-ac20-4fdcb9d4ccdb\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T13:37:04.992090Z\",\"iopub.execute_input\":\"2024-05-25T13:37:04.992370Z\",\"iopub.status.idle\":\"2024-05-25T13:37:05.292551Z\",\"shell.execute_reply.started\":\"2024-05-25T13:37:04.992345Z\",\"shell.execute_reply\":\"2024-05-25T13:37:05.291621Z\"}}\ntrain_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'Split-train')\n\ntfrecord_filenames = [os.path.join(train_uri, name) for name in os.listdir(train_uri)]\n\ndataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type = 'GZIP')\n\nfor tfrecord in dataset.take(2):\n  serialized_example = tfrecord.numpy()\n  example = tf.train.Example()\n  example.ParseFromString(serialized_example)\n  pp.pprint(example)\n\n# %% [markdown] {\"id\":\"Ha_HNrUBOxGx\"}\n# \n# ## 1.2 Data Validation (StatisticsGen)\n\n# %% [code] {\"id\":\"VsywedEyOavc\",\"outputId\":\"df8eb9af-9d5e-40ec-ed6a-819dd95b7bda\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T13:37:05.293947Z\",\"iopub.execute_input\":\"2024-05-25T13:37:05.294404Z\",\"iopub.status.idle\":\"2024-05-25T13:37:09.478350Z\",\"shell.execute_reply.started\":\"2024-05-25T13:37:05.294370Z\",\"shell.execute_reply\":\"2024-05-25T13:37:09.477469Z\"}}\nstatistics_gen = StatisticsGen(\n    examples=example_gen.outputs[\"examples\"]\n)\n\n\ninteractive_context.run(statistics_gen)\n\n# %% [code] {\"id\":\"hw1qkMUyOteX\",\"outputId\":\"a56258a5-5e0a-4219-bf3e-3593c92a6908\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T13:37:09.479939Z\",\"iopub.execute_input\":\"2024-05-25T13:37:09.480585Z\",\"iopub.status.idle\":\"2024-05-25T13:37:09.516611Z\",\"shell.execute_reply.started\":\"2024-05-25T13:37:09.480550Z\",\"shell.execute_reply\":\"2024-05-25T13:37:09.515605Z\"}}\ninteractive_context.show(statistics_gen.outputs[\"statistics\"])\n\n# %% [markdown] {\"id\":\"o1t_kY3nRhhj\"}\n# ## 1.3 Data Validation (Data Scheme)\n\n# %% [code] {\"id\":\"nI_TT9udOta8\",\"outputId\":\"17253aa7-6457-455f-c029-eda07e2fc7e5\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T13:37:09.517828Z\",\"iopub.execute_input\":\"2024-05-25T13:37:09.518115Z\",\"iopub.status.idle\":\"2024-05-25T13:37:09.601987Z\",\"shell.execute_reply.started\":\"2024-05-25T13:37:09.518090Z\",\"shell.execute_reply\":\"2024-05-25T13:37:09.601095Z\"}}\nschema_gen = SchemaGen(\n    statistics = statistics_gen.outputs['statistics']\n)\n\ninteractive_context.run(schema_gen)\n\n# %% [code] {\"id\":\"X7AgeS1nOtX4\",\"outputId\":\"70c73cca-6d1d-4801-f794-3e9eea0de268\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T13:37:09.603015Z\",\"iopub.execute_input\":\"2024-05-25T13:37:09.603313Z\",\"iopub.status.idle\":\"2024-05-25T13:37:09.622120Z\",\"shell.execute_reply.started\":\"2024-05-25T13:37:09.603266Z\",\"shell.execute_reply\":\"2024-05-25T13:37:09.621252Z\"}}\ninteractive_context.show(schema_gen.outputs[\"schema\"])\n\n# %% [markdown] {\"id\":\"B8fBKaD8U_5s\"}\n# ## 1.4 Anomaly Detection (ExampleValidator)\n\n# %% [code] {\"id\":\"pk18Oq0sU1VS\",\"outputId\":\"5d6e0287-d0b7-4ad9-8bf9-441451259245\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T13:37:09.623417Z\",\"iopub.execute_input\":\"2024-05-25T13:37:09.624059Z\",\"iopub.status.idle\":\"2024-05-25T13:37:09.708990Z\",\"shell.execute_reply.started\":\"2024-05-25T13:37:09.624021Z\",\"shell.execute_reply\":\"2024-05-25T13:37:09.708078Z\"}}\nexample_validator = ExampleValidator(\n    statistics = statistics_gen.outputs['statistics'],\n    schema     = schema_gen.outputs['schema']\n)\n\ninteractive_context.run(example_validator)\n\n# %% [code] {\"id\":\"ZcpzmbahVFmD\",\"outputId\":\"78768c18-1c0b-44dd-beff-9684e30cd3e6\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T13:37:09.710204Z\",\"iopub.execute_input\":\"2024-05-25T13:37:09.710554Z\",\"iopub.status.idle\":\"2024-05-25T13:37:09.728775Z\",\"shell.execute_reply.started\":\"2024-05-25T13:37:09.710522Z\",\"shell.execute_reply\":\"2024-05-25T13:37:09.727897Z\"}}\ninteractive_context.show(example_validator.outputs['anomalies'])\n\n# %% [markdown] {\"id\":\"mHr5ahVgVXf3\"}\n# # Data Preprocessing\n\n# %% [markdown] {\"id\":\"JZ_0xjPAVaSV\"}\n# ## 2.1 Transform\n\n# %% [code] {\"id\":\"1CUuwKcbdUJL\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T13:37:09.733200Z\",\"iopub.execute_input\":\"2024-05-25T13:37:09.733479Z\",\"iopub.status.idle\":\"2024-05-25T13:37:09.737193Z\",\"shell.execute_reply.started\":\"2024-05-25T13:37:09.733455Z\",\"shell.execute_reply\":\"2024-05-25T13:37:09.736293Z\"}}\nTRANSFORM_MODULE_FILE = 'Amazon_review_classification_transform.py'\n\n# %% [code] {\"id\":\"YzRXOpakVJaL\",\"outputId\":\"1a35e61b-53ea-402b-f23d-c3e3a6f83205\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T13:37:09.738382Z\",\"iopub.execute_input\":\"2024-05-25T13:37:09.738659Z\",\"iopub.status.idle\":\"2024-05-25T13:37:09.746417Z\",\"shell.execute_reply.started\":\"2024-05-25T13:37:09.738636Z\",\"shell.execute_reply\":\"2024-05-25T13:37:09.745410Z\"}}\n%%writefile {TRANSFORM_MODULE_FILE}\nimport tensorflow as tf\nimport os\nfrom nltk.corpus import stopwords\nimport nltk\n\nnltk.download('stopwords')\n\nLABEL_KEY   = \"Label\"\nFEATURE_KEY = \"Text\"\n\n# Define the set of stopwords\nstop_words = set(stopwords.words('english'))\n\n# Renaming transformed features\ndef transformed_name(key):\n    return key + \"_xf\"\n\n# Preprocess input features into transformed features\ndef preprocessing_fn(inputs):\n    \"\"\"\n    inputs:  map from feature keys to raw features\n    outputs: map from feature keys to transformed features\n    \"\"\"\n\n    outputs = {}\n\n    # Convert text to lowercase\n    text_lower = tf.strings.lower(inputs[FEATURE_KEY])\n\n    # Remove stopwords\n    text_without_stopwords = tf.strings.regex_replace(text_lower, '|'.join(stop_words), '')\n\n    # Store transformed text feature\n    outputs[transformed_name(FEATURE_KEY)] = text_without_stopwords\n\n    outputs[transformed_name(LABEL_KEY)]   = tf.cast(inputs[LABEL_KEY], tf.int64)\n\n\n    return outputs\n\n\n# %% [code] {\"id\":\"k38ya7yDd9X4\",\"outputId\":\"b231760e-0705-43f1-e257-320bb6a3631b\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T13:37:09.749507Z\",\"iopub.execute_input\":\"2024-05-25T13:37:09.749791Z\",\"iopub.status.idle\":\"2024-05-25T13:37:55.385602Z\",\"shell.execute_reply.started\":\"2024-05-25T13:37:09.749768Z\",\"shell.execute_reply\":\"2024-05-25T13:37:55.384646Z\"}}\ntransform = Transform(\n    examples    = example_gen.outputs['examples'],\n    schema      = schema_gen.outputs['schema'],\n    module_file = os.path.abspath(TRANSFORM_MODULE_FILE)\n)\n\ninteractive_context.run(transform)\n\n# %% [markdown] {\"id\":\"zqTx9dIDhY4x\"}\n# # Model Training\n\n# %% [markdown] {\"id\":\"Bxr8MRU9hiSz\"}\n# ## 3.1 Tuner\n\n# %% [code] {\"id\":\"CCKHPYf_ETzG\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T13:37:55.386936Z\",\"iopub.execute_input\":\"2024-05-25T13:37:55.387295Z\",\"iopub.status.idle\":\"2024-05-25T13:37:55.391639Z\",\"shell.execute_reply.started\":\"2024-05-25T13:37:55.387247Z\",\"shell.execute_reply\":\"2024-05-25T13:37:55.390820Z\"}}\nTUNER_MODULE_FILE = 'Amazon_review_classification_tuner.py'\n\n# %% [code] {\"id\":\"3dCyyQeEhrl7\",\"outputId\":\"0d19059f-0119-4ac5-b9fa-dff9466b3f63\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T14:39:28.497675Z\",\"iopub.execute_input\":\"2024-05-25T14:39:28.498380Z\",\"iopub.status.idle\":\"2024-05-25T14:39:28.508272Z\",\"shell.execute_reply.started\":\"2024-05-25T14:39:28.498348Z\",\"shell.execute_reply\":\"2024-05-25T14:39:28.507323Z\"}}\n%%writefile {TUNER_MODULE_FILE}\nimport os\nimport tensorflow as tf\nimport tensorflow_transform as tft\nimport keras_tuner as kt\nfrom tensorflow.keras import layers\nfrom tfx.components.trainer.fn_args_utils import FnArgs\nfrom keras_tuner.engine import base_tuner\nfrom typing import NamedTuple, Dict, Text, Any\n\nLABEL_KEY   = 'Label'\nFEATURE_KEY = 'Text'\n\ndef transformed_name(key):\n    \"\"\"Renaming transformed features\"\"\"\n    return key + \"_xf\"\n\ndef gzip_reader_fn(filenames):\n    \"\"\"Loads compressed data\"\"\"\n    return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n\ndef input_fn(file_pattern, tf_transform_output, num_epochs, batch_size=128) -> tf.data.Dataset:\n    \"\"\"Get post_tranform feature & create batches of data\"\"\"\n    \n    transform_feature_spec = (\n        tf_transform_output.transformed_feature_spec().copy()\n    )\n    \n    # create batches of data\n    dataset = tf.data.experimental.make_batched_features_dataset(\n        file_pattern = file_pattern,\n        batch_size   = batch_size,\n        features     = transform_feature_spec,\n        reader       = gzip_reader_fn,\n        num_epochs   = num_epochs,\n        label_key    = transformed_name(LABEL_KEY)\n    )\n\n    return dataset\n\n# Vocabulary size and number of words in a sequence.\nVOCAB_SIZE      = 1000\nSEQUENCE_LENGTH = 900\n\nvectorize_layer = layers.TextVectorization(\n    standardize            = 'lower_and_strip_punctuation',\n    max_tokens             = VOCAB_SIZE,\n    output_mode            = 'int',\n    output_sequence_length = SEQUENCE_LENGTH\n)\n\ndef model_builder(hp):\n    \"\"\"Build keras tuner model\"\"\"\n    embedding_dim = hp.Int('embedding_dim', min_value=30, max_value=40, step=5)\n    lstm_units    = hp.Int('lstm_units', min_value=48, max_value=96, step=16)\n    dropout_rate = hp.Float('dropout_rate', min_value=0.3, max_value=0.5, step=0.1)\n    learning_rate = hp.Choice('learning_rate', values=[1e-2])\n    \n    inputs = tf.keras.Input(shape=(1,), name=transformed_name(FEATURE_KEY), dtype=tf.string)\n    \n    reshaped_narrative = tf.reshape(inputs, [-1])\n    x = vectorize_layer(reshaped_narrative)\n    x = layers.Embedding(VOCAB_SIZE, embedding_dim, name='embedding')(x)\n    x = layers.Dropout(dropout_rate)(x)\n    x = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True))(x)\n    x = layers.Bidirectional(layers.LSTM(lstm_units))(x)\n    x = layers.Dropout(dropout_rate)(x)\n    x = layers.BatchNormalization()(x)\n    outputs = layers.Dense(4, activation='softmax')(x)\n    \n    model = tf.keras.Model(inputs = inputs, outputs = outputs)\n    model.compile(\n        loss      = tf.keras.losses.SparseCategoricalCrossentropy(),\n        optimizer = tf.keras.optimizers.Adam(learning_rate),\n        metrics   = ['accuracy']\n    )\n    \n    model.summary()\n    return model\n\nTunerFnResult = NamedTuple('TunerFnResult', [\n    ('tuner', base_tuner.BaseTuner),\n    ('fit_kwargs', Dict[Text, Any]),\n])\n\nearly_stop_callback = tf.keras.callbacks.EarlyStopping(\n    monitor  = 'val_accuracy',\n    mode     = 'max',\n    verbose  = 1,\n    patience = 2\n)\n\ndef tuner_fn(fn_args: FnArgs) -> None:\n    # Load the transform output\n    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n    \n    # Create batches of data\n    train_set = input_fn(fn_args.train_files[0], tf_transform_output, 5)\n    val_set   = input_fn(fn_args.eval_files[0],  tf_transform_output, 5)\n\n    vectorize_layer.adapt(\n        [j[0].numpy()[0] for j in [\n            i[0][transformed_name(FEATURE_KEY)]\n                for i in list(train_set)\n        ]]\n    )\n    \n    # Build the model tuner\n    model_tuner = kt.Hyperband(\n        hypermodel   = lambda hp: model_builder(hp),\n        objective    = kt.Objective('val_accuracy', direction='max'),\n        max_epochs   = 5,\n        factor       = 2,\n        directory    = fn_args.working_dir,\n        project_name = 'amazon_review_classification'\n    )\n    model_tuner.oracle.max_trials = 10\n\n    return TunerFnResult(\n        tuner      = model_tuner,\n        fit_kwargs = {\n            'callbacks'        : [early_stop_callback],\n            'x'                : train_set,\n            'validation_data'  : val_set,\n            'steps_per_epoch'  : fn_args.train_steps,\n            'validation_steps' : fn_args.eval_steps\n        }\n    )\n     \n\n# %% [code] {\"id\":\"nDsLK4lJjwK-\",\"outputId\":\"c5b800c2-6f24-4b24-9c36-392f14a237a6\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T14:39:29.514623Z\",\"iopub.execute_input\":\"2024-05-25T14:39:29.515416Z\",\"iopub.status.idle\":\"2024-05-25T15:45:15.178679Z\",\"shell.execute_reply.started\":\"2024-05-25T14:39:29.515374Z\",\"shell.execute_reply\":\"2024-05-25T15:45:15.177739Z\"}}\ntuner = Tuner(\n    module_file     = os.path.abspath(TUNER_MODULE_FILE),\n    examples        = transform.outputs['transformed_examples'],\n    transform_graph = transform.outputs['transform_graph'],\n    schema          = schema_gen.outputs['schema'],\n    train_args      = trainer_pb2.TrainArgs(splits=['train']),\n    eval_args       = trainer_pb2.EvalArgs(splits=['eval'])\n)\n\ninteractive_context.run(tuner)\n\n# %% [markdown]\n# ## 3.2 Training\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-05-25T17:11:39.325089Z\",\"iopub.execute_input\":\"2024-05-25T17:11:39.326115Z\",\"iopub.status.idle\":\"2024-05-25T17:11:39.330233Z\",\"shell.execute_reply.started\":\"2024-05-25T17:11:39.326076Z\",\"shell.execute_reply\":\"2024-05-25T17:11:39.329345Z\"}}\nTRAINER_MODULE_FILE = 'Amazon_review_classification_trainer.py'\n\n# %% [code] {\"id\":\"IkuWfqMSozfZ\",\"execution\":{\"iopub.status.busy\":\"2024-05-25T17:26:26.113345Z\",\"iopub.execute_input\":\"2024-05-25T17:26:26.113633Z\",\"iopub.status.idle\":\"2024-05-25T17:26:26.122516Z\",\"shell.execute_reply.started\":\"2024-05-25T17:26:26.113608Z\",\"shell.execute_reply\":\"2024-05-25T17:26:26.121580Z\"}}\n%%writefile {TRAINER_MODULE_FILE}\nimport os\nimport tensorflow as tf\nimport tensorflow_transform as tft\nfrom tensorflow.keras import layers\nfrom tfx.components.trainer.fn_args_utils import FnArgs\n\nLABEL_KEY   = 'Label'\nFEATURE_KEY = 'Text'\n\ndef transformed_name(key):\n    \"\"\"Renaming transformed features\"\"\"\n    return key + \"_xf\"\n\ndef gzip_reader_fn(filenames):\n    \"\"\"Loads compressed data\"\"\"\n    return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n\ndef input_fn(file_pattern, tf_transform_output, num_epochs, batch_size=128) -> tf.data.Dataset:\n    \"\"\"Get post_tranform feature & create batches of data\"\"\"\n    \n    transform_feature_spec = (\n        tf_transform_output.transformed_feature_spec().copy()\n    )\n    \n    # create batches of data\n    dataset = tf.data.experimental.make_batched_features_dataset(\n        file_pattern = file_pattern,\n        batch_size   = batch_size,\n        features     = transform_feature_spec,\n        reader       = gzip_reader_fn,\n        num_epochs   = num_epochs,\n        label_key    = transformed_name(LABEL_KEY)\n    )\n\n    return dataset\n\n# Vocabulary size and number of words in a sequence\nVOCAB_SIZE      = 10000\nSEQUENCE_LENGTH = 900\nembedding_dim   = 35\n\nvectorize_layer = layers.TextVectorization(\n    standardize            = 'lower_and_strip_punctuation',\n    max_tokens             = VOCAB_SIZE,\n    output_mode            = 'int',\n    output_sequence_length = SEQUENCE_LENGTH\n)\n\ndef model_builder(hp):\n    \"\"\"Build keras tuner model\"\"\"\n    inputs = tf.keras.Input(shape=(1,), name=transformed_name(FEATURE_KEY), dtype=tf.string)\n    \n    reshaped_narrative = tf.reshape(inputs, [-1])\n    x = vectorize_layer(reshaped_narrative)\n    x = layers.Embedding(VOCAB_SIZE, hp['embedding_dim'], name='embedding')(x)\n    x = layers.Dropout(hp['dropout_rate'])(x)\n    x = layers.Bidirectional(layers.LSTM(hp['lstm_units'], return_sequences=True))(x)\n    x = layers.Bidirectional(layers.LSTM(hp['lstm_units']))(x)\n    x = layers.Dropout(hp['dropout_rate'])(x)\n    x = layers.BatchNormalization()(x)\n    outputs = layers.Dense(4, activation='softmax')(x)\n    \n    model = tf.keras.Model(inputs = inputs, outputs = outputs)\n    model.compile(\n        loss      = tf.keras.losses.SparseCategoricalCrossentropy(),\n        optimizer = tf.keras.optimizers.Adam(hp['learning_rate']),\n        metrics   = ['accuracy']\n    )\n    \n    model.summary()\n    return model\n\ndef _get_serve_tf_examples_fn(model, tf_transform_output):\n    model.tft_layer = tf_transform_output.transform_features_layer()\n    \n    @tf.function\n    def serve_tf_examples_fn(serialized_tf_examples):\n        feature_spec = tf_transform_output.raw_feature_spec()\n        feature_spec.pop(LABEL_KEY)\n\n        parsed_features      = tf.io.parse_example(serialized_tf_examples, feature_spec)\n        transformed_features = model.tft_layer(parsed_features)\n        \n        # get predictions using the transformed features\n        return model(transformed_features)\n        \n    return serve_tf_examples_fn\n    \ndef run_fn(fn_args: FnArgs) -> None:\n    log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), 'logs')\n    hp      = fn_args.hyperparameters['values']\n    hp['epochs'] = 10\n    \n    \n    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n        log_dir = log_dir, update_freq='batch'\n    )\n    \n    early_stop_callback = tf.keras.callbacks.EarlyStopping(\n        monitor  = 'val_binary_accuracy',\n        mode     = 'max',\n        verbose  = 1,\n        patience = 10\n    )\n\n    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n        fn_args.serving_model_dir,\n        monitor        = 'val_binary_accuracy',\n        mode           = 'max',\n        verbose        = 1,\n        save_best_only = True\n    )\n\n    callbacks = [\n        tensorboard_callback,\n        early_stop_callback,\n        model_checkpoint_callback\n    ]\n    \n    # Load the transform output\n    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n    \n    # Create batches of data\n    train_set = input_fn(fn_args.train_files, tf_transform_output, hp['tuner/epochs'])\n    val_set   = input_fn(fn_args.eval_files,  tf_transform_output, hp['tuner/epochs'])\n\n    vectorize_layer.adapt(\n        [j[0].numpy()[0] for j in [\n            i[0][transformed_name(FEATURE_KEY)]\n                for i in list(train_set)\n        ]]\n    )\n    \n    # Build the model\n    model = model_builder(hp)\n    \n    # Train the model\n    model.fit(\n        x                = train_set,\n        validation_data  = val_set,\n        callbacks        = callbacks,\n        steps_per_epoch  = fn_args.train_steps,\n        validation_steps = fn_args.eval_steps,\n        epochs           = hp['epochs']\n    )\n\n    signatures = {\n        'serving_default': _get_serve_tf_examples_fn(\n            model, tf_transform_output\n        ).get_concrete_function(\n            tf.TensorSpec(\n                shape = [None],\n                dtype = tf.string,\n                name  = 'examples'\n            )\n        )\n    }\n\n    model.save(\n        fn_args.serving_model_dir,\n        save_format = 'tf',\n        signatures  = signatures\n    )\n     \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-05-25T17:26:26.123522Z\",\"iopub.execute_input\":\"2024-05-25T17:26:26.123848Z\",\"iopub.status.idle\":\"2024-05-25T17:45:07.419348Z\",\"shell.execute_reply.started\":\"2024-05-25T17:26:26.123813Z\",\"shell.execute_reply\":\"2024-05-25T17:45:07.418256Z\"}}\ntrainer = Trainer(\n    module_file     = os.path.abspath(TRAINER_MODULE_FILE),\n    examples        = transform.outputs['transformed_examples'],\n    transform_graph = transform.outputs['transform_graph'],\n    schema          = schema_gen.outputs['schema'],\n    hyperparameters = tuner.outputs['best_hyperparameters'],\n    train_args      = trainer_pb2.TrainArgs(splits=['train']),\n    eval_args       = trainer_pb2.EvalArgs(splits=['eval'])\n)\n\ninteractive_context.run(trainer)\n\n# %% [markdown]\n# # Model Analysis & Validation\n# ## 4.1 Resolver\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-05-25T18:16:27.695167Z\",\"iopub.execute_input\":\"2024-05-25T18:16:27.695916Z\",\"iopub.status.idle\":\"2024-05-25T18:16:27.754539Z\",\"shell.execute_reply.started\":\"2024-05-25T18:16:27.695883Z\",\"shell.execute_reply\":\"2024-05-25T18:16:27.753659Z\"}}\nmodel_resolver = Resolver(\n    strategy_class = LatestBlessedModelStrategy,\n    model          = Channel(type=Model),\n    model_blessing = Channel(type=ModelBlessing)\n).with_id('Latest_blessed_model_resolver')\n\ninteractive_context.run(model_resolver)\n\n# %% [markdown]\n# ## 4.2 Evaluator\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-05-25T18:24:45.341203Z\",\"iopub.execute_input\":\"2024-05-25T18:24:45.341984Z\",\"iopub.status.idle\":\"2024-05-25T18:24:45.349465Z\",\"shell.execute_reply.started\":\"2024-05-25T18:24:45.341952Z\",\"shell.execute_reply\":\"2024-05-25T18:24:45.348609Z\"}}\neval_config = tfma.EvalConfig(\n    model_specs   = [tfma.ModelSpec(label_key = 'Label')],\n    slicing_specs = [tfma.SlicingSpec()],\n    metrics_specs = [\n        tfma.MetricsSpec(metrics=[\n            tfma.MetricConfig(class_name = 'ExampleCount'),\n            tfma.MetricConfig(class_name = 'AUC'),\n            tfma.MetricConfig(class_name = 'FalsePositives'),\n            tfma.MetricConfig(class_name = 'TruePositives'),\n            tfma.MetricConfig(class_name = 'FalseNegatives'),\n            tfma.MetricConfig(class_name = 'TrueNegatives'),\n            tfma.MetricConfig(class_name = 'BinaryAccuracy',\n                threshold=tfma.MetricThreshold(\n                    value_threshold = tfma.GenericValueThreshold(\n                        lower_bound = {'value': 0.5}\n                    ),\n                    change_threshold = tfma.GenericChangeThreshold(\n                        direction = tfma.MetricDirection.HIGHER_IS_BETTER,\n                        absolute  = {'value': 0.0001}\n                    )\n                )\n            )\n        ])\n    ]\n)\n     \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-05-25T18:24:46.019777Z\",\"iopub.execute_input\":\"2024-05-25T18:24:46.020152Z\",\"iopub.status.idle\":\"2024-05-25T18:25:29.467825Z\",\"shell.execute_reply.started\":\"2024-05-25T18:24:46.020124Z\",\"shell.execute_reply\":\"2024-05-25T18:25:29.466949Z\"}}\nevaluator = Evaluator(\n    examples       = example_gen.outputs['examples'],\n    model          = trainer.outputs['model'],\n    baseline_model = model_resolver.outputs['model'],\n    eval_config    = eval_config\n)\n\ninteractive_context.run(evaluator)\n     \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-05-25T18:26:18.188407Z\",\"iopub.execute_input\":\"2024-05-25T18:26:18.188799Z\",\"iopub.status.idle\":\"2024-05-25T18:26:18.206128Z\",\"shell.execute_reply.started\":\"2024-05-25T18:26:18.188768Z\",\"shell.execute_reply\":\"2024-05-25T18:26:18.205326Z\"}}\neval_result = evaluator.outputs['evaluation'].get()[0].uri\ntfma_result = tfma.load_eval_result(eval_result)\ntfma.view.render_slicing_metrics(tfma_result)\ntfma.addons.fairness.view.widget_view.render_fairness_indicator(tfma_result)\n\n# %% [markdown]\n# # MODEL DEPLOYMENT\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-05-25T18:32:18.744119Z\",\"iopub.execute_input\":\"2024-05-25T18:32:18.744875Z\",\"iopub.status.idle\":\"2024-05-25T18:32:18.843116Z\",\"shell.execute_reply.started\":\"2024-05-25T18:32:18.744840Z\",\"shell.execute_reply\":\"2024-05-25T18:32:18.842098Z\"}}\nfrom tfx.components import Pusher\nfrom tfx.proto import pusher_pb2\n\npusher = Pusher(\n    model            = trainer.outputs['model'],\n    model_blessing   = evaluator.outputs['blessing'],\n    push_destination = pusher_pb2.PushDestination(\n        filesystem = pusher_pb2.PushDestination.Filesystem(\n            base_directory = 'serving_model_dir/Amazon_review_classification')\n    )\n)\n\ninteractive_context.run(pusher)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-05-25T18:35:36.743994Z\",\"iopub.execute_input\":\"2024-05-25T18:35:36.744681Z\",\"iopub.status.idle\":\"2024-05-25T18:35:46.221471Z\",\"shell.execute_reply.started\":\"2024-05-25T18:35:36.744648Z\",\"shell.execute_reply\":\"2024-05-25T18:35:46.220184Z\"}}\n!zip -r pipelines.zip pipelines/\n!zip -r serving_model_dir.zip serving_model_dir/\n!pip freeze > requirements.txt\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-05-25T19:40:11.896877Z\",\"iopub.execute_input\":\"2024-05-25T19:40:11.897436Z\",\"iopub.status.idle\":\"2024-05-25T19:40:11.985631Z\",\"shell.execute_reply.started\":\"2024-05-25T19:40:11.897399Z\",\"shell.execute_reply\":\"2024-05-25T19:40:11.984073Z\"}}\nasd\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [code]\n","metadata":{"_uuid":"729fe5c9-57e0-4611-96e3-96c4fb145ed9","_cell_guid":"5f76521d-1b02-4be5-9df6-31126c0e8c8b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}